<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value><%= @rm_private_ip %>:8031</value>
    <description>host is the hostname of the resource manager and port is the port on which the NodeManagers contact the Resource Manager.</description>
  </property>
   <property>
    <name>yarn.resourcemanager.resource-tracker.port</name>
    <value>8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value><%= @rm_private_ip %>:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.port</name>
    <value>8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value><%= @rm_private_ip %>:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.port</name>
    <value>8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value><%= @rm_private_ip %>:8033</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.port</name>
    <value>8033</value>
  </property>
  <property>
    <name>yarn.resourcemanager.groupMembership.address</name>
    <value><%= @rm_private_ip %>:8034</value>
  </property>
  <property>
    <name>yarn.resourcemanager.groupMembership.port</name>
    <value>8034</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value><%= @rm_private_ip %>:8088</value>
    <description>The http address of the RM web application.</description>
  </property>

  <property>
    <name>yarn.nodemanager.delete.debug-delay-sec</name>
    <value><%= node['hops']['container_cleanup_delay_sec'] %></value>
  </property>
  <property>
    <description>NM Webapp address.</description>
    <name>yarn.nodemanager.webapp.address</name>
    <value><%= @my_private_ip %>:8042</value>
  </property>
  <property>
    <name>yarn.nodemanager.address</name>
    <value><%= @my_private_ip %>:9000</value>
  </property>
  <property>
    <name>yarn.nodemanager.localizer.address</name>
    <value><%= @my_private_ip %>:9001</value>
  </property>
  <property>
    <name>yarn.application.classpath</name>
    <value><%= node['hops']['yarn']['app_classpath'] %></value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value><%= node['hops']['rm']['scheduler_class'] %></value>
    <description>In case you do not want to use the default scheduler</description>
  </property>
  <property>
    <name>hadoop.registry.zk.quorum</name>
    <value><%= @zk_ip %>:2181</value>
  </property>
  <property>
    <name>yarn.nodemanager.container-executor.class</name>
    <value><%= node['hops']['yarn']['container_executor'] %></value>
    <description>LinuxContainerExecutor uses CGroups, DefaultContainerExecutor uses Unix processes.</description>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.group</name>
    <value><%= node['hops']['group'] %></value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.resources-handler.class</name>
    <value><%= @resource_handler %></value>
  </property>
  <property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms</name>
    <value><%= node['hops']['yarn']['cgroups_deletion_timeout'] %></value>
    <description>Timeout for deleting Cgroups after container is finished.</description>
  </property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.hierarchy</name>
    <value>hops-yarn</value>
    <description>LinuxContainerExecutor CGroups hierarchy.</description>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.mount-path</name>
    <value>/sys/fs/cgroup</value>
    <description>LinuxContainerExecutor CGroups mount-path.</description>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.mount</name>
    <value>false</value>
    <description>Whether the LCE should attempt to mount cgroups if not found. Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler.</description>
  </property>
  <property>
    <name>yarn.nodemanager.resource.percentage-physical-cpu-limit</name>
    <value><%= node['hops']['yarn']['cgroups_max_cpu_usage'] %></value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage</name>
    <value><%= node['hops']['yarn']['cgroups_strict_resource_usage'] %></value>
  </property>
   <property>
    <name>yarn.resourcemanager.scheduler.client.thread-count</name>
    <value>50</value>
    <description>Number of ResourceManager threads for decoding and handling client RPCs for apps.</description>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.client.thread-count</name>
    <value>50</value>
    <description>Number of ResourceTracker threads for decoding and handling client RPCs for NodeManager heartbeats.</description>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.client.thread-count</name>
    <value>1</value>
    <description>Number of ResourceManager threads for decoding and handling client RPCs for Admin operations.</description>
  </property>
  <property>
    <name>yarn.nodemanager.container-manager.thread-count</name>
    <value>20</value>
    <description>Number of threads for starting/stopping containers</description>
  </property>
  <property>
    <name>yarn.nodemanager.local-dirs</name>
    <value><%= node['hops']['tmp_dir'] %>/nm-local-dir</value>
    <description>the local directories used by the nodemanager to store its localized files</description>
  </property>

  <property>
    <name>yarn.nodemanager.log-dirs</name>
    <value><%= node['hops']['logs_dir'] %>/userlogs</value>
    <description>the local directories used by the nodemanager to store its localized logs</description>
  </property>
  <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value><%= node['hops']['yarn']['nodemanager']['remote_app_log_dir'] %></value>
    <description>the directory where the logs are aggregated</description>
  </property>

  <%# <property> %>
  <%#   <name>yarn.nodemanager.aux-services</name> %>
  <%#   <value><%= node['hops']['yarn']['aux_services'] %1></value> %>
  <%#   <description>shuffle service that needs to be set for Spark external shuffle service (dynamic allocators) and  MapReduce to run</description> %>
  <%# </property> %>
  <%# <property> %>
  <%#   <name>yarn.nodemanager.aux-services.spark_shuffle.class</name> %>
  <%#   <value>org.apache.spark.network.yarn.YarnShuffleService</value> %>
  <%#   <description>The exact name of the class for the Spark external shuffle service</description> %>
  <%# </property> %>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value><%= node['hops']['mr']['shuffle_class'] %></value>
    <description>The exact name of the class for shuffle service</description>
  </property>

  <property>
    <name>yarn.nodemanager.vmem-pmem-ratio</name>

    <value><%= node['hops']['yarn']['vpmem_ratio'] %></value>
    <description>The virtual memory (physical + paged memory) upper limit for each Map and Reduce task is determined by the virtual memory ratio each YARN Container is allowed.</description>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value><%= node['hops']['yarn']['vmem_check'] %></value>
    <description></description>
  </property>
  <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value><%= node['hops']['yarn']['pmem_check'] %></value>
    <description></description>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value><%= node['hops']['yarn']['memory_mbs'] %></value>
    <description>the amount of memory on the NodeManager in MB</description>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>128</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value><%= node['hops']['yarn']['max_allocation_memory_mb'] %></value>
    <description>The maximum allocation for every container request at the RM, in MBs. Memory requests higher than this will throw a InvalidResourceRequestException.</description>
  </property>
  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value><%= node['hops']['yarn']['vcores'] %></value>
    <description>Number of CPU cores that can be allocated for containers.</description>
  </property>

  <% if node['hops']['yarn']['detect-hardware-capabilities'].casecmp?("true") -%>
  <property>
    <name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
    <value>true</value>
    <description>Enable auto-detection of node capabilities such as memory and CPU.</description>
  </property>

  <property>
    <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
    <value><%= node['hops']['yarn']['pcores-vcores-multiplier'] %></value>
    <description>Multiplier to determine how to convert phyiscal cores to vcores. This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and 
    yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The number of vcores will be calculated as number of CPUs * multiplier.</description>
  </property>

  <property>
    <name>yarn.nodemanager.resource.count-logical-processors-as-cores</name>
    <value><%= node['hops']['yarn']['logical-processors-as-cores'] %></value>
    <description>Flag to determine if logical processors(such as hyperthreads) should be counted as cores. Only applicable on Linux when yarn.nodemanager.resource.cpu-vcores is set to -1 and 
    yarn.nodemanager.resource.detect-hardware-capabilities is true.</description>
  </property>

  <property>
    <name>yarn.nodemanager.resource.system-reserved-memory-mb</name>
    <value><%= node['hops']['yarn']['system-reserved-memory-mb'] %></value>
    <description>Amount of physical memory, in MB, that is reserved for non-YARN processes. This configuration is only used if yarn.nodemanager.resource.detect-hardware-capabilities is set to true and 
    yarn.nodemanager.resource.memory-mb is -1. If set to -1, this amount is calculated as 20% of (system memory - 2*HADOOP_HEAPSIZE)</description>
  </property>
  <% end %>

  <property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value><%= node['hops']['yarn']['min_vcores'] %></value>
    <description>The minimum allocation for every container request at the RM, in terms of virtual CPU cores. Requests lower than this won't take effect, and the specified value will get allocated the minimum.</description>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value><%= node['hops']['yarn']['max_vcores'] %></value>
    <description>The maximum allocation for every container request at the RM, in terms of virtual CPU cores. Requests higher than this won't take effect, and will get capped to this value.</description>
  </property>
  <property>
    <name>yarn.nodemanager.log.retain-seconds</name>
    <value><%= node['hops']['yarn']['log_retain_secs'] %></value>
    <description>Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled.</description>
  </property>
  <property>
    <name>yarn.resourcemanager.am.max-retries</name>
    <value><%= node['hops']['am']['max_retries'] %></value>
    <description>Number of times to try to restart the ApplicationMaster by the ResourceManager if its NodeManager fails.</description>
  </property>
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value><%= node['hops']['yarn']['log_aggregation'] %></value>
    <description>Enable Default aggregatioin of log files HDFS, deleting them from the NodeManager.</description>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value><%= node['hops']['yarn']['log_retain_secs'] %></value>
    <description>Default time (in seconds) to retain log files in HDFS. Only applicable if log-aggregation is disabled.</description>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-check-interval-seconds</name>
    <value><%= node['hops']['yarn']['log_retain_check'] %></value>
    <description>Default time (in seconds) between checks for retained log files in HDFS. Only applicable if log-aggregation is disabled.</description>
  </property>
  <property>
    <name>yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds</name>
    <value><%= node['hops']['yarn']['log_roll_interval'] %></value>
    <description>Defines how often NMs wake up to upload log files. The default value is -1. By default, the logs will be uploaded when the application is finished. By setting this configure, logs can be uploaded periodically when the application is running. The minimum rolling-interval-seconds can be set is 3600.
    </description>
  </property>
  <property>
    <name>yarn.nodemanager.heartbeat.interval-ms</name>
    <value><%= node['hops']['yarn']['nodemanager_hb_ms'] %></value>
    <description>Time in ms between heartbeats sent from the NodeManager to the ResourceManager.</description>
  </property>

  <!--HA-->
  <property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value><%= node['hops']['yarn']['nodemanager_ha_enabled'] %></value>
  </property>
 <property>
    <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
    <value><%= node['hops']['yarn']['nodemanager_auto_failover_enabled'] %></value>
  </property>
  <property>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value><%= node['hops']['yarn']['nodemanager_recovery_enabled'] %></value>
  </property>
  <property>
    <name>yarn.nodemanager.recovery.dir</name>
    <value><%= node['hops']['yarn']['nodemanager_recovery_dir'] %></value>
  </property>

  <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value><%= node['hops']['rm']['private_ips'].join(",") %></value>
  </property>


<% for id in node['hops']['rm']['private_ips'] -%>
  <property>
    <name>yarn.resourcemanager.hostname.<%= id %></name>
    <value><%= id %></value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address.<%= id %></name>
    <value><%= id %>:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address.<%= id %></name>
    <value><%= id %>:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address.<%= id %></name>
    <value><%= id %>:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address.<%= id %></name>
    <value><%= id %>:8033</value>
  </property>
  <property>
    <name>yarn.resourcemanager.groupMembership.address.<%= id %></name>
    <value><%= id %>:8034</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address.<%= id %></name>
    <value><%= id %>:8088</value>
  </property>

<% end -%>

  <property>
    <name>yarn.resourcemanager.store.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.DBRMStateStore</value>
  </property>
  <property>
    <name>dfs.leader.check.interval</name>
    <value><%= node['hops']['yarn']['rm_heartbeat'] %></value>
  </property>

  <!--ndb-->
  <!--batching rpcs for commiting to db-->
  <property>
      <name>hops.yarn.resourcemanager.batch.max.size</name>
      <value><%= node['hops']['yarn']['nodemanager_rpc_batch_max_size'] %></value>
  </property>
  <property>
      <name>hops.yarn.resourcemanager.batch.max.duration</name>
      <value><%= node['hops']['yarn']['nodemanager_rpc_batch_max_duration'] %></value>
  </property>
  <!--distributed mode-->
  <property>
    <name>yarn.client.failover-distributed</name>
    <value><%= node['hops']['yarn']['rm_distributed'] %></value>
  </property>

  <property>
      <name>hops.yarn.resourcemanager.ndb-event-streaming.enable</name>
      <value><%= node['hops']['yarn']['nodemanager_rm_streaming_enabled'] %></value>
  </property>

  <property>
    <name>yarn.client.failover-sleep-base-ms</name>
    <value><%= node['hops']['yarn']['client_failover_sleep_base_ms'] %></value>
  </property>

  <property>
    <name>yarn.client.failover-sleep-max-ms</name>
    <value><%= node['hops']['yarn']['client_failover_sleep_max_ms'] %></value>
  </property>

  <!-- quotas -->
  <property>
    <name>yarn.quotas.enabled</name>
    <value><%= node['hops']['yarn']['quota_enabled'] %></value>
  </property>
  <property>
    <name>yarn.quotas.containers-logs.monitor-interval</name>
    <value><%= node['hops']['yarn']['quota_monitor_interval'] %></value>
  </property>
  <property>
    <name>yarn.quotas.ticks.per.credit</name>
    <value><%= node['hops']['yarn']['quota_ticks_per_credit'] %></value>
  </property>
  <property>
    <name>yarn.quotas.ticks.per.credit</name>
    <value><%= node['hops']['yarn']['quota_min_ticks_charge'] %></value>
  </property>
  <property>
    <name>yarn.quotas.containers-logs.checkpoints-minticks</name>
    <value><%= node['hops']['yarn']['quota_checkpoint_nbticks'] %></value>
  </property>
  <property>
    <name>yarn.resourcemanager.quota.multiplicator.threshold.gpu</name>
    <value><%= node['hops']['yarn']['quota_threshold_gpu'] %></value>
  </property>
  <property>
    <name>yarn.resourcemanager.quota.minimum.charged.mb</name>
    <value><%= node['hops']['yarn']['quota_minimum_charged_mb'] %></value>
  </property>
  <property>
    <name>yarn.resourcemanager.quota.variable.price.enabled</name>
    <value><%= node['hops']['yarn']['quota_variable_price_enabled'] %></value>
  </property>


  <property>
    <name>yarn.resourcemanager.proxy-user-privileges.enabled</name>
    <value>true</value>
  </property>

<!-- uber pricing -->
<property>
    <name>yarn.quotas.overpricing-threshold.mb</name>
    <value>0.1</value>
</property>

<property>
    <name>yarn.quotas.overpricing-threshold.mb</name>
    <value>0.1</value>
</property>

<!-- SSL -->
<!--
<property>
    <name>yarn.http.policy</name>
    <value><%= node['hops']['yarn']['http']['policy'] %></value>
</property>
<property>
    <name>yarn.log.server.url</name>
    <value><%= node['hops']['yarn']['log']['server']['url'] %></value>
</property>
<property>
    <name>yarn.resourcemanager.webapp.https.address</name>
    <value><%= node['hops']['yarn']['resourcemanager']['webapp']['https']['address'] %></value>
</property>
<property>
    <name>yarn.nodemanager.webapp.https.address</name>
    <value><%= node['hops']['yarn']['nodemanager']['webapp']['https']['address'] %></value>
</property>
-->

<!-- container monitoring and metrics period -->
<property>
  <name>yarn.nodemanager.container-metrics.period-ms</name>
  <value>2000</value>
  <description>Container metrics flush period in ms. Set to -1 for flush on completion.</description>
</property>

<property>
  <name>yarn.nodemanager.container-monitor.interval-ms</name>
  <value>2000</value>
  <description>How often to monitor containers.</description>
</property>

<property>
  <name>yarn.nodemanager.user-home-dir</name>
  <value><%= node['hops']['yarnapp']['home_dir'] %></value>
</property>

<!-- GPU settings -->
<property>
    <name>yarn.scheduler.minimum-allocation-gpus</name>
    <value><%= node['hops']['yarn']['min_gpus'] %></value>
</property>
<property>
    <name>yarn.scheduler.maximum-allocation-gpus</name>
    <value><%= node['hops']['yarn']['max_gpus'] %></value>
</property>
<property>
    <name>yarn.nodemanager.resource.gpus.enabled</name>
    <value><%= node['hops']['gpu'] %></value>
</property>
<property>
    <name>yarn.nodemanager.resource.gpus</name>
    <value><%= @num_gpus %></value>
</property>
<property>
    <name>yarn.nodemanager.gpu.management-impl</name>
    <value><%= node['hops']['yarn']['gpu_impl_class'] %></value>
</property>
<property>
    <name>yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user</name>
    <value><%= node['hops']['yarnapp']['user'] %></value>
</property>
<property>
  <name>yarn.nodemanager.disk-health-checker.enable</name>
  <value>true</value>
</property>

<!-- RMAppSecurity x.509 and JWT -->
<property>
  <name>yarn.resourcemanager.rmappsecurity.actor-class</name>
  <value><%= node['hops']['rmappsecurity']['actor_class'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.x509.sign-path</name>
  <value><%= node['hops']['rmappsecurity']['x509']['sign-path'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.x509.revoke-path</name>
  <value><%= node['hops']['rmappsecurity']['x509']['revoke-path'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.x509.expiration-safety-period</name>
  <value><%= node['hops']['rmappsecurity']['x509']['expiration_safety_period'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.x509.revocation-monitor-interval</name>
  <value><%= node['hops']['rmappsecurity']['x509']['revocation_monitor_interval'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.jwt.enabled</name>
  <value><%= node['hops']['rmappsecurity']['jwt']['enabled'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.jwt.validity</name>
  <value><%= node['hops']['rmappsecurity']['jwt']['validity'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.jwt.expiration-leeway</name>
  <value><%= node['hops']['rmappsecurity']['jwt']['expiration-leeway'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.jwt.audience</name>
  <value><%= node['hops']['rmappsecurity']['jwt']['audience'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.jwt.generate-path</name>
  <value><%= node['hops']['rmappsecurity']['jwt']['generate-path'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.jwt.invalidate-path</name>
  <value><%= node['hops']['rmappsecurity']['jwt']['invalidate-path'] %></value>
</property>

<property>
  <name>yarn.resourcemanager.rmappsecurity.jwt.renew-path</name>
  <value><%= node['hops']['rmappsecurity']['jwt']['renew-path'] %></value>
</property>

<!--<property>
    <name>yarn.nodemanager.linux-container-executor.nonsecure-mode.limit.users</name>
    <value><%= node['hops']['yarn']['linux_container_limit_users'] %></value>
</property>-->

</configuration>
